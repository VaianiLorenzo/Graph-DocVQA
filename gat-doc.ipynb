{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:12:27.933439Z",
     "iopub.status.busy": "2025-02-02T15:12:27.933121Z",
     "iopub.status.idle": "2025-02-02T15:12:32.176217Z",
     "shell.execute_reply": "2025-02-02T15:12:32.175431Z",
     "shell.execute_reply.started": "2025-02-02T15:12:27.933401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os, pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:12:32.177792Z",
     "iopub.status.busy": "2025-02-02T15:12:32.177376Z",
     "iopub.status.idle": "2025-02-02T15:12:32.182018Z",
     "shell.execute_reply": "2025-02-02T15:12:32.180958Z",
     "shell.execute_reply.started": "2025-02-02T15:12:32.177767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#para\n",
    "top_k_smilarity = 5\n",
    "loss_set = 'bce'    # 'bce', 'focal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change here the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:12:32.183253Z",
     "iopub.status.busy": "2025-02-02T15:12:32.182947Z",
     "iopub.status.idle": "2025-02-02T15:12:54.503985Z",
     "shell.execute_reply": "2025-02-02T15:12:54.503155Z",
     "shell.execute_reply.started": "2025-02-02T15:12:32.183228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# OCR doc_info\n",
    "with open('/kaggle/input/doc-info/train_doc_info.pkl','rb') as f:\n",
    "    train_doc_info = pickle.load(f)\n",
    "with open('/kaggle/input/doc-info/val_doc_info.pkl','rb') as f:\n",
    "    val_doc_info = pickle.load(f)\n",
    "\n",
    "#embeddings for doc object\n",
    "with open('/kaggle/input/e5v-embeddings/train_E5Vembeddings.pkl','rb') as f:\n",
    "    train_doc_emb = pickle.load(f)\n",
    "with open('/kaggle/input/e5v-embeddings/val_E5Vembeddings.pkl','rb') as f:\n",
    "    val_doc_emb = pickle.load(f)\n",
    "\n",
    "#embeddings for question (no ans)\n",
    "with open('/kaggle/input/e5v-qaemb/train_e5-v_QAemb.pkl','rb') as f:\n",
    "    train_question_emb= pickle.load(f)\n",
    "    # train_df = train_df[['question', 'global_id','pmcid']]\n",
    "    # train_df['global_id'] = train_df['global_id'].apply(ast.literal_eval)\n",
    "with open('/kaggle/input/e5v-qaemb/val_e5-v_QAemb.pkl','rb') as f:\n",
    "    val_question_emb = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:12:54.506418Z",
     "iopub.status.busy": "2025-02-02T15:12:54.506039Z",
     "iopub.status.idle": "2025-02-02T15:12:59.388492Z",
     "shell.execute_reply": "2025-02-02T15:12:59.387491Z",
     "shell.execute_reply.started": "2025-02-02T15:12:54.506390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/doc-info/test_doc_info.pkl','rb') as f:\n",
    "    test_doc_info = pickle.load(f)\n",
    "\n",
    "with open('/kaggle/input/e5v-embeddings/test_E5Vembeddings.pkl','rb') as f:\n",
    "    test_doc_emb = pickle.load(f)\n",
    "\n",
    "with open('/kaggle/input/e5v-qaemb/test_e5-v_QAemb.pkl','rb') as f:\n",
    "    test_question_emb = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OCR doc info from local(page) to global(doc)** \n",
    "\n",
    "parents & children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:12:59.391103Z",
     "iopub.status.busy": "2025-02-02T15:12:59.390687Z",
     "iopub.status.idle": "2025-02-02T15:12:59.404172Z",
     "shell.execute_reply": "2025-02-02T15:12:59.402708Z",
     "shell.execute_reply.started": "2025-02-02T15:12:59.391061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_infomation(doc_info):\n",
    "    global_id_list = []\n",
    "    relation_list = []\n",
    "    position_list = []\n",
    "    # local(page level) relation to global(doc level) relation\n",
    "    for name in tqdm(doc_info):\n",
    "        global_order_id_list = []\n",
    "        xml_json = doc_info[name]\n",
    "        parent_child_dict = {}\n",
    "        parent_box = None\n",
    "        iterate_id = 0\n",
    "        id = 0\n",
    "\n",
    "        #Iterate the pages in each doc\n",
    "        for p in xml_json['pages']:\n",
    "            page = xml_json['pages'][p]\n",
    "            order_obj = page['ordered_id']\n",
    "            for order_id in order_obj:\n",
    "                global_order = int(order_id) + iterate_id\n",
    "                global_order_id_list.append(global_order)\n",
    "                \n",
    "            for obj in page['objects']:\n",
    "                box = page['objects'][obj]\n",
    "                local_id = int(box['id'])\n",
    "    \n",
    "                # 如果box的category_id为2，则保存并迭代\n",
    "                if box['category_id'] == 2:\n",
    "                    parent_box = (box['bbox'], local_id + iterate_id)\n",
    "                # 如果box的category_id为1且box[relations]为空list，则储存的cate为2的为parent\n",
    "                elif box['category_id'] == 1 and not box['relations']:\n",
    "                    if parent_box is not None:\n",
    "                        parent_id = parent_box[1]\n",
    "                        if parent_id in parent_child_dict:\n",
    "                            parent_child_dict[parent_id].append([\"parent\", str(local_id + iterate_id)])\n",
    "                        else:\n",
    "                            parent_child_dict[parent_id] = [[\"parent\", str(local_id + iterate_id)]]\n",
    "                        parent_child_dict[local_id + iterate_id] = [[\"child\", str(parent_id)]]\n",
    "                id += 1\n",
    "            iterate_id = id\n",
    "        \n",
    "        id = 0\n",
    "        iterate_id = 0\n",
    "        i=0\n",
    "        for page in doc_info[name]['pages']:\n",
    "            page_info = doc_info[name]['pages'][page]\n",
    "            for obj in page_info['objects']:\n",
    "                objt = page_info['objects'][obj]\n",
    "                global_id_list.append(objt['global_id'])\n",
    "        \n",
    "                global_relations = []\n",
    "                global_gap_dict = {}\n",
    "                \n",
    "                relations = objt['relations']\n",
    "                # gaps = objt['gap']\n",
    "                \n",
    "                for relation in relations:\n",
    "                    # if relation[0] == 'child':\n",
    "                        global_relation=int(relation[1])+iterate_id\n",
    "                        global_relations.append(global_relation)\n",
    "                \n",
    "                # Adding parent_child_dict info to global_relations\n",
    "                if id in parent_child_dict:\n",
    "                    for relation in parent_child_dict[id]:\n",
    "                        # if relation[0] == 'child':\n",
    "                            global_relations.append(relation[1])\n",
    "                global_relations = [int(x) for x in global_relations] \n",
    "                relation_list.append(global_relations)\n",
    "\n",
    "                # 查找当前元素在列表中的位置\n",
    "                index = global_order_id_list.index(id)\n",
    "                \n",
    "                if index == 0:\n",
    "                    position_list.append([global_order_id_list[index + 1]])\n",
    "                elif index == len(global_order_id_list) - 1:\n",
    "                    position_list.append([global_order_id_list[index - 1]])\n",
    "                else:\n",
    "                    position_list.append([global_order_id_list[index - 1], global_order_id_list[index + 1]])\n",
    "\n",
    "                    \n",
    "                id+=1\n",
    "            iterate_id=id\n",
    "            i+=1\n",
    "\n",
    "    return global_id_list, relation_list, position_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:12:59.405886Z",
     "iopub.status.busy": "2025-02-02T15:12:59.405445Z",
     "iopub.status.idle": "2025-02-02T15:13:00.444939Z",
     "shell.execute_reply": "2025-02-02T15:13:00.443742Z",
     "shell.execute_reply.started": "2025-02-02T15:12:59.405846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "global_id_list, relation_list, position_list = combine_infomation(train_doc_info)\n",
    "train_doc_emb['global_id'] = global_id_list\n",
    "train_doc_emb['relations'] = relation_list\n",
    "train_doc_emb['neighbor'] = position_list\n",
    "\n",
    "global_id_list, relation_list, position_list = combine_infomation(val_doc_info)\n",
    "val_doc_emb['global_id'] = global_id_list\n",
    "val_doc_emb['relations'] = relation_list\n",
    "val_doc_emb['neighbor'] = position_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:13:00.446423Z",
     "iopub.status.busy": "2025-02-02T15:13:00.445977Z",
     "iopub.status.idle": "2025-02-02T15:13:01.293914Z",
     "shell.execute_reply": "2025-02-02T15:13:01.292714Z",
     "shell.execute_reply.started": "2025-02-02T15:13:00.446384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_doc_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:13:01.295482Z",
     "iopub.status.busy": "2025-02-02T15:13:01.295075Z",
     "iopub.status.idle": "2025-02-02T15:13:02.031762Z",
     "shell.execute_reply": "2025-02-02T15:13:02.030544Z",
     "shell.execute_reply.started": "2025-02-02T15:13:01.295444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_doc_emb[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:13:02.033208Z",
     "iopub.status.busy": "2025-02-02T15:13:02.032834Z",
     "iopub.status.idle": "2025-02-02T15:13:09.319623Z",
     "shell.execute_reply": "2025-02-02T15:13:09.318478Z",
     "shell.execute_reply.started": "2025-02-02T15:13:02.033170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:13:09.321617Z",
     "iopub.status.busy": "2025-02-02T15:13:09.321160Z",
     "iopub.status.idle": "2025-02-02T15:13:12.808044Z",
     "shell.execute_reply": "2025-02-02T15:13:12.807081Z",
     "shell.execute_reply.started": "2025-02-02T15:13:09.321572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:13:12.809602Z",
     "iopub.status.busy": "2025-02-02T15:13:12.809085Z",
     "iopub.status.idle": "2025-02-02T15:13:12.816478Z",
     "shell.execute_reply": "2025-02-02T15:13:12.815338Z",
     "shell.execute_reply.started": "2025-02-02T15:13:12.809574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def similarity_edges(obj_embeddings, top_k):\n",
    "    \n",
    "    norms = obj_embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "    similarities = torch.matmul(obj_embeddings, obj_embeddings.T) / (norms * norms.T)\n",
    "    edges = []\n",
    "    for i in range(len(obj_embeddings)):\n",
    "        i_similarities = similarities[i]\n",
    "        top_k_values, top_k_indices = torch.topk(i_similarities, k=top_k)\n",
    "        \n",
    "        for j, sim in zip(top_k_indices, top_k_values):\n",
    "            edges.append((i, j.item(), sim.item()))\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:13:12.818701Z",
     "iopub.status.busy": "2025-02-02T15:13:12.818093Z",
     "iopub.status.idle": "2025-02-02T15:13:12.840176Z",
     "shell.execute_reply": "2025-02-02T15:13:12.839082Z",
     "shell.execute_reply.started": "2025-02-02T15:13:12.818655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def relation_edges(obj_embeddings, relations):\n",
    "    \n",
    "    edges = []\n",
    "    for i, relas in enumerate(relations):\n",
    "        for j in range(obj_embeddings.size(0)):\n",
    "            if i != j:\n",
    "                if j in relas:\n",
    "                    edges.append((i, j, 1))\n",
    "            else:\n",
    "                edges.append((i, j, 1))  # (source_node, target_node, similarity, relation)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:13:12.844960Z",
     "iopub.status.busy": "2025-02-02T15:13:12.844599Z",
     "iopub.status.idle": "2025-02-02T15:14:23.682399Z",
     "shell.execute_reply": "2025-02-02T15:14:23.681150Z",
     "shell.execute_reply.started": "2025-02-02T15:13:12.844933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "train_edges_per_doc = {}\n",
    "train_sim_per_doc = {}\n",
    "train_neighbor_per_doc = {}\n",
    "\n",
    "grouped = train_doc_emb.groupby('doc')\n",
    "\n",
    "for doc_id, group in tqdm(grouped):\n",
    "    doc_embeddings = torch.cat([torch.tensor(e) for e in group['embeddings']], dim=0)    \n",
    "    doc_relations = group['relations']\n",
    "    doc_neighbor = group['neighbor']\n",
    "    train_edges_per_doc[doc_id] = relation_edges(doc_embeddings, doc_relations)\n",
    "    train_sim_per_doc[doc_id] = similarity_edges(doc_embeddings, top_k_smilarity)\n",
    "    train_neighbor_per_doc[doc_id] = relation_edges(doc_embeddings, doc_neighbor)\n",
    "   \n",
    "val_edges_per_doc = {}\n",
    "val_sim_per_doc = {}\n",
    "val_neighbor_per_doc = {}\n",
    "\n",
    "# 使用 groupby 根据 doc_id 进行分组\n",
    "grouped = val_doc_emb.groupby('doc')\n",
    "\n",
    "for doc_id, group in tqdm(grouped):\n",
    "    doc_embeddings = torch.cat([torch.tensor(e) for e in group['embeddings']], dim=0)    \n",
    "    doc_relations = group['relations']\n",
    "    doc_neighbor = group['neighbor']\n",
    "    val_edges_per_doc[doc_id] = relation_edges(doc_embeddings, doc_relations)\n",
    "    val_sim_per_doc[doc_id] = similarity_edges(doc_embeddings, top_k_smilarity)\n",
    "    val_neighbor_per_doc[doc_id] = relation_edges(doc_embeddings, doc_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.685125Z",
     "iopub.status.busy": "2025-02-02T15:14:23.684853Z",
     "iopub.status.idle": "2025-02-02T15:14:23.690613Z",
     "shell.execute_reply": "2025-02-02T15:14:23.689517Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.685101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for key, value in val_edges_per_doc.items():\n",
    "    if key == '27760561':\n",
    "        print(value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.691846Z",
     "iopub.status.busy": "2025-02-02T15:14:23.691495Z",
     "iopub.status.idle": "2025-02-02T15:14:23.714444Z",
     "shell.execute_reply": "2025-02-02T15:14:23.713350Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.691820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PygData(Data):\n",
    "    def __init__(self, question_embedding, object_embeddings, edge_index1, edge_attr1, edge_index2, edge_attr2, edge_index3, edge_attr3, target):\n",
    "        super().__init__()\n",
    "        self.question_embedding = question_embedding\n",
    "        self.object_embeddings = object_embeddings\n",
    "        self.edge_index1 = edge_index1\n",
    "        self.edge_attr1 = edge_attr1\n",
    "        self.edge_index2 = edge_index2\n",
    "        self.edge_attr2 = edge_attr2\n",
    "        self.edge_index3 = edge_index3\n",
    "        self.edge_attr3 = edge_attr3\n",
    "        self.target = target\n",
    "\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        # Increment indices for edge_index1 and edge_index2 (pyg data: only 1 edge default)\n",
    "        if key in ['edge_index1', 'edge_index2', 'edge_index3']:\n",
    "            return self.object_embeddings.size(0)  # Increment by the number of nodes\n",
    "        return super().__inc__(key, value, *args, **kwargs)\n",
    "        \n",
    "class QAGNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, question_embed, object_embeddings, edges_per_doc, sim_per_doc, neighbor_per_doc):\n",
    "        self.question_embeddings = question_embed.question\n",
    "        self.answer_ids = question_embed.global_id\t\n",
    "        self.qa_doc_ids = question_embed.pmcid\n",
    "        # self.obj_doc = object_embeddings.doc\n",
    "        # self.object_embeddings = object_embeddings.embeddings\n",
    "        self.obj_doc = object_embeddings\n",
    "        self.edges_per_doc = edges_per_doc\n",
    "        self.sim_per_doc = sim_per_doc\n",
    "        self.neighbor_per_doc = neighbor_per_doc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.question_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' \n",
    "        we have to make graph of every question and the objects, so dataloader helped indict the question using idx and we match the doc to find the objects\n",
    "        '''\n",
    "        question_embedding = self.question_embeddings[idx].float()\n",
    "        answer_ids = self.answer_ids[idx]\n",
    "        answer_doc_id = self.qa_doc_ids[idx]\n",
    "        \n",
    "        answer_doc_id = str(answer_doc_id)\n",
    "        answer_ids = ast.literal_eval(answer_ids)\n",
    "        \n",
    "        # qa --> doc_id -->edges\n",
    "        doc_edges = self.edges_per_doc[answer_doc_id]\n",
    "        doc_sim = self.sim_per_doc[answer_doc_id]\n",
    "        doc_neighbor = self.neighbor_per_doc[answer_doc_id]\n",
    "\n",
    "        # edges --> edges index and attr\n",
    "        edge_index1 = torch.tensor([[edge[0], edge[1]] for edge in doc_edges], dtype=torch.long).t().contiguous()\n",
    "        edge_attr1 = torch.tensor([edge[2] for edge in doc_edges], dtype=torch.long)\n",
    "\n",
    "        edge_index2 = torch.tensor([[edge[0], edge[1]] for edge in doc_sim], dtype=torch.long).t().contiguous()\n",
    "        edge_attr2 = torch.tensor([edge[2] for edge in doc_sim], dtype=torch.float)\n",
    "\n",
    "        edge_index3 = torch.tensor([[edge[0], edge[1]] for edge in doc_neighbor], dtype=torch.long).t().contiguous()\n",
    "        edge_attr3 = torch.tensor([edge[2] for edge in doc_neighbor], dtype=torch.long)\n",
    "        \n",
    "        # doc_id --> node feature\n",
    "        doc_mask = self.obj_doc['doc'] == answer_doc_id\n",
    "        filtered_embeddings = self.obj_doc.loc[doc_mask, 'embeddings']\n",
    "        object_embeddings = torch.stack(list(filtered_embeddings)).squeeze(1).float()  #[x, 4096]\n",
    "\n",
    "        # add QUESTION NODE at the end \n",
    "        qa_embeddings = torch.cat([object_embeddings, question_embedding], dim=0) \n",
    "        \n",
    "        # Index of the QUESTION NODE\n",
    "        question_node_idx = len(filtered_embeddings)  \n",
    "        num_objects = len(filtered_embeddings)\n",
    "        question_to_object_edges = torch.tensor([[question_node_idx] * num_objects, torch.arange(num_objects)]).long()\n",
    "        \n",
    "        edge_index1 = torch.cat([edge_index1, question_to_object_edges], dim=1)\n",
    "        edge_index2 = torch.cat([edge_index2, question_to_object_edges], dim=1)\n",
    "        edge_index3 = torch.cat([edge_index3, question_to_object_edges], dim=1)\n",
    "\n",
    "        # attr of the QUESTION NODE \n",
    "        similarities_question_to_object = torch.matmul(question_embedding, object_embeddings.t()).squeeze(0)\n",
    "        similarities_question_to_object = torch.tensor(similarities_question_to_object, dtype=torch.float)\n",
    "        edge_attr_question_to_object = torch.ones(len(filtered_embeddings),  dtype=torch.float)\n",
    "        \n",
    "        edge_attr1 = torch.cat([edge_attr1, edge_attr_question_to_object], dim=0)\n",
    "        edge_attr2 = torch.cat([edge_attr2, similarities_question_to_object], dim=0)        \n",
    "        edge_attr3 = torch.cat([edge_attr3, edge_attr_question_to_object], dim=0)\n",
    "\n",
    "        # LABEL\n",
    "        # doc_id --> node `global_id`\n",
    "        object_global_id = self.obj_doc.loc[doc_mask, 'global_id']\n",
    "        object_global_id = torch.tensor(object_global_id.values, dtype=torch.long)\n",
    "                \n",
    "        target = torch.zeros(object_global_id.size(0), dtype=torch.float)\n",
    "        i = 0 \n",
    "        for id in object_global_id:\n",
    "            if id in answer_ids:  \n",
    "                target[i] = 1\n",
    "            i += 1\n",
    "\n",
    "        return PygData(question_embedding, qa_embeddings, edge_index1, edge_attr1, edge_index2, edge_attr2, edge_index3, edge_attr3, target)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.715881Z",
     "iopub.status.busy": "2025-02-02T15:14:23.715560Z",
     "iopub.status.idle": "2025-02-02T15:14:23.741373Z",
     "shell.execute_reply": "2025-02-02T15:14:23.740020Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.715855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "val_dataset = QAGNNDataset(val_question_emb, val_doc_emb, val_edges_per_doc, val_sim_per_doc, val_neighbor_per_doc)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset = QAGNNDataset(train_question_emb, train_doc_emb, train_edges_per_doc, train_sim_per_doc, train_neighbor_per_doc)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.742955Z",
     "iopub.status.busy": "2025-02-02T15:14:23.742542Z",
     "iopub.status.idle": "2025-02-02T15:14:23.763138Z",
     "shell.execute_reply": "2025-02-02T15:14:23.762062Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.742916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.764423Z",
     "iopub.status.busy": "2025-02-02T15:14:23.764127Z",
     "iopub.status.idle": "2025-02-02T15:14:23.823234Z",
     "shell.execute_reply": "2025-02-02T15:14:23.822315Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.764400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_dataloader):\n",
    "    print(f\"Batch {idx + 1}: {batch}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.824498Z",
     "iopub.status.busy": "2025-02-02T15:14:23.824190Z",
     "iopub.status.idle": "2025-02-02T15:14:23.843384Z",
     "shell.execute_reply": "2025-02-02T15:14:23.842248Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.824461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for el in val_dataloader:\n",
    "    print(el['edge_index1'])\n",
    "    # print(el['edge_attr3'])\n",
    "    # print(el['question_embedding'].shape)\n",
    "    # print(el['object_embeddings'].shape)\n",
    "    # print(el['edge_attr'].shape)\n",
    "    # print(len(el['answer_ids']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.844851Z",
     "iopub.status.busy": "2025-02-02T15:14:23.844473Z",
     "iopub.status.idle": "2025-02-02T15:14:23.848829Z",
     "shell.execute_reply": "2025-02-02T15:14:23.847746Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.844813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #uncommemt the print line in the QAGNNDataset and print , change the doc name to check the logic\n",
    "# for index, line in val_doc_emb.iterrows():\n",
    "#     if line['doc'] == '29609541':\n",
    "#         fist_emb = line['embeddings'][0]\n",
    "#         fist_id = line['global_id']\n",
    "#         print(f'In doc 29609541, the first embeddings is {fist_emb},global_id is {fist_id}')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.850363Z",
     "iopub.status.busy": "2025-02-02T15:14:23.850020Z",
     "iopub.status.idle": "2025-02-02T15:14:23.879139Z",
     "shell.execute_reply": "2025-02-02T15:14:23.877852Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.850330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "\n",
    "class GATConvC(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, head_count=4, aggr='add'):\n",
    "        super().__init__(aggr=aggr)  \n",
    "        self.sim_weights = nn.Parameter(torch.full((1,), 0.5))\n",
    "        self.head_count = head_count\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dim_per_head = out_channels // head_count\n",
    "\n",
    "        assert out_channels % head_count == 0, \"out_channels must be divisible by head_count\"\n",
    "\n",
    "        self.linear_key = nn.Linear(in_channels + 1, head_count * self.dim_per_head)\n",
    "        self.linear_query = nn.Linear(in_channels, head_count * self.dim_per_head)\n",
    "        self.linear_msg = nn.Linear(in_channels + 1, head_count * self.dim_per_head)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(out_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "\n",
    "        aggr_out = self.propagate(edge_index, x=x, edge_attr=edge_attr) #[E, emb_dim]\n",
    "        out = self.mlp(aggr_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr, edge_index):\n",
    "\n",
    "        # edge_attr = self.sim_weights * edge_attr[:,0] + (1 - self.sim_weights) * edge_attr[:,1]\n",
    "        # edge_attr = edge_attr[:,1]\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "\n",
    "        edge_features = torch.cat([x_i, x_j, edge_attr], dim=1)  \n",
    "\n",
    "        key_input = torch.cat([x_i, edge_attr], dim=1)  \n",
    "        msg_input = torch.cat([x_j, edge_attr], dim=1)  \n",
    "        # Linear transformations\n",
    "        key = self.linear_key(key_input).view(-1, self.head_count, self.dim_per_head)  \n",
    "        msg = self.linear_msg(msg_input).view(-1, self.head_count, self.dim_per_head) \n",
    "        query = self.linear_query(x_j).view(-1, self.head_count, self.dim_per_head) \n",
    "        \n",
    "        # attn score\n",
    "        scores = (query * key).sum(dim=-1)\n",
    "        scores = scores / (self.dim_per_head ** 0.5)  \n",
    "        # scores = F.leaky_relu(scores, negative_slope=0.2)\n",
    "        alpha = softmax(scores, edge_index[0])\n",
    "        #added attr inside\n",
    "        # alpha = edge_attr * alpha\n",
    "        # alpha = torch.sigmoid(alpha)\n",
    "        self._alpha = alpha\n",
    "        \n",
    "        out = msg * alpha.unsqueeze(-1)\n",
    "        return out.view(-1, self.out_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.880384Z",
     "iopub.status.busy": "2025-02-02T15:14:23.880051Z",
     "iopub.status.idle": "2025-02-02T15:14:23.900787Z",
     "shell.execute_reply": "2025-02-02T15:14:23.899738Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.880359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gelu(x)\n",
    "\n",
    "class QAGNN_Message_Passing(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation=GELU(), dropout=0.1):\n",
    "        super(QAGNN_Message_Passing, self).__init__()\n",
    "        # Using GATConv (could replace with GATv2Conv if needed)\n",
    "        self.gat = GATConvC(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.act = activation\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.gat(x, edge_index, edge_attr)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class QAGNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, k, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vecmap = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.gnn = QAGNN_Message_Passing(hidden_size, hidden_size, activation=GELU())\n",
    "        self.linear = nn.Linear(output_size, 1)\n",
    "        self.weight_sim = nn.Parameter(torch.tensor(0.5))\n",
    "        self.weight_rela = nn.Parameter(torch.tensor(0.5))\n",
    "    \n",
    "    def forward(self, node_features, edge_index1, edge_attr1, edge_index2, edge_attr2):\n",
    "        node_features = self.vecmap(node_features)\n",
    "\n",
    "        node_sim_conv = self.gnn(node_features, edge_index2, edge_attr2)\n",
    "        node_rela_conv = self.gnn(node_features, edge_index1, edge_attr1)\n",
    "\n",
    "        node_score_sim = self.linear(node_sim_conv)\n",
    "        node_score_rela = self.linear(node_rela_conv)\n",
    "        node_score = self.weight_sim * node_score_sim + self.weight_rela * node_score_rela\n",
    "        \n",
    "        return node_score[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.902149Z",
     "iopub.status.busy": "2025-02-02T15:14:23.901870Z",
     "iopub.status.idle": "2025-02-02T15:14:23.926813Z",
     "shell.execute_reply": "2025-02-02T15:14:23.925624Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.902123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_exact_match_ratio(true_labels, predicted_labels):\n",
    "    exact_match = 0\n",
    "    for i in range(len(true_labels)):\n",
    "      if true_labels[i] == predicted_labels[i]:\n",
    "        exact_match += 1\n",
    "    return exact_match / len(true_labels)\n",
    "\n",
    "def calculate_target_match_ratio(true_labels, predicted_labels):\n",
    "    exact_match = 0\n",
    "    for i in range(len(true_labels)):\n",
    "      if true_labels[i] == predicted_labels[i] == 1.0:\n",
    "        exact_match += 1\n",
    "                  \n",
    "    return exact_match / np.sum(true_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.928670Z",
     "iopub.status.busy": "2025-02-02T15:14:23.928177Z",
     "iopub.status.idle": "2025-02-02T15:14:23.954814Z",
     "shell.execute_reply": "2025-02-02T15:14:23.953682Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.928631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    exact_match_sum = 0\n",
    "    target_match_sum = []\n",
    "    total_batches = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            object_embeddings = batch.object_embeddings.to(device)\n",
    "            edge_index1 = batch.edge_index1.to(device) #relation \n",
    "            edge_attr1 = batch.edge_attr1.to(device)\n",
    "            edge_index2 = batch.edge_index2.to(device) #similarity\n",
    "            edge_attr2 = batch.edge_attr2.to(device)\n",
    "            edge_index3 = batch.edge_index3.to(device) #neighbor\n",
    "            edge_attr3 = batch.edge_attr3.to(device)\n",
    "            question_embedding = batch.question_embedding.to(device)\n",
    "            target = batch.target.to(device)\n",
    "    \n",
    "            # optimizer.zero_grad()\n",
    "            logits = model(object_embeddings, edge_index1, edge_attr1, edge_index2, edge_attr2)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            preds = (probabilities >= 0.5).float()\n",
    "            \n",
    "            loss = loss_fn(logits.squeeze(), target)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Exact Match of every ele\n",
    "            exact_match = calculate_exact_match_ratio(target, preds)\n",
    "            exact_match_sum += exact_match\n",
    "\n",
    "            # Positive Recall\n",
    "            if torch.sum(target).item() > 0: \n",
    "                target_match = calculate_target_match_ratio(target, preds)\n",
    "                target_match_sum.append(target_match)\n",
    "\n",
    "            total_batches += 1\n",
    "\n",
    "            predict_list.append(preds.squeeze().to(\"cpu\").tolist())\n",
    "            target_list.append(target.to(\"cpu\").to(torch.int).tolist())\n",
    "\n",
    "    avg_loss = running_loss / total_batches\n",
    "    avg_val_acc = exact_match_sum / total_batches\n",
    "    avg_pos_recall = sum(target_match_sum) / len(target_match_sum) if target_match_sum else 0.0\n",
    "    epoch_accu = calculate_exact_match_ratio(target_list, predict_list)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}],Val Avg Loss: {avg_loss:.4f}, Val Accuracy: {avg_val_acc:.4f}, Val Recall: {avg_pos_recall:.4f}, Val Exact match:{epoch_accu:.4f}\")\n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.956336Z",
     "iopub.status.busy": "2025-02-02T15:14:23.955929Z",
     "iopub.status.idle": "2025-02-02T15:14:23.978788Z",
     "shell.execute_reply": "2025-02-02T15:14:23.977635Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.956297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        \n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        pt = probabilities * targets + (1 - probabilities) * (1 - targets)\n",
    "        focal_loss = (1 - pt) ** self.gamma * bce_loss\n",
    "        \n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:23.980422Z",
     "iopub.status.busy": "2025-02-02T15:14:23.980045Z",
     "iopub.status.idle": "2025-02-02T15:14:23.998569Z",
     "shell.execute_reply": "2025-02-02T15:14:23.997549Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.980394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class weighted_bce_loss(nn.Module):\n",
    "    def __init__(self, weight=2.0):\n",
    "        self.weight = weight\n",
    "            \n",
    "    def forward(self, logits, targets):\n",
    "        pos_weight = torch.tensor([self.weight], dtype=torch.float32, device=logits.device)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets, pos_weight=pos_weight)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:24.000004Z",
     "iopub.status.busy": "2025-02-02T15:14:23.999646Z",
     "iopub.status.idle": "2025-02-02T15:14:25.058078Z",
     "shell.execute_reply": "2025-02-02T15:14:25.056769Z",
     "shell.execute_reply.started": "2025-02-02T15:14:23.999968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = 4096\n",
    "hidden_size = 4096\n",
    "k = 1  # Number of GAT layers\n",
    "\n",
    "# pos_weight = torch.tensor([2.0]).to(device)\n",
    "model = QAGNN(input_size, hidden_size,  hidden_size, k).to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "if loss_set == 'bce':\n",
    "    loss_fn=nn.BCEWithLogitsLoss()\n",
    "if loss_set == 'focal':\n",
    "    loss_fn = FocalLoss(gamma=2)\n",
    "if loss_set == 'weighted_bce':\n",
    "    loss_fn = weighted_bce_loss(weight=2.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:25.059473Z",
     "iopub.status.busy": "2025-02-02T15:14:25.059079Z",
     "iopub.status.idle": "2025-02-02T15:14:25.065726Z",
     "shell.execute_reply": "2025-02-02T15:14:25.064641Z",
     "shell.execute_reply.started": "2025-02-02T15:14:25.059441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:25.067176Z",
     "iopub.status.busy": "2025-02-02T15:14:25.066904Z",
     "iopub.status.idle": "2025-02-02T15:14:25.082754Z",
     "shell.execute_reply": "2025-02-02T15:14:25.081545Z",
     "shell.execute_reply.started": "2025-02-02T15:14:25.067154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# start_model_path = '/kaggle/input/3graph-mlp/best_model (4).pth'\n",
    "# model.load_state_dict(torch.load(start_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:25.084355Z",
     "iopub.status.busy": "2025-02-02T15:14:25.083944Z",
     "iopub.status.idle": "2025-02-02T15:20:04.468883Z",
     "shell.execute_reply": "2025-02-02T15:20:04.467125Z",
     "shell.execute_reply.started": "2025-02-02T15:14:25.084314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 15\n",
    "all_pred = {}\n",
    "best_val_acc = 0.0  \n",
    "best_model_path = \"/kaggle/working/best_model.pth\" \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    exact_match_sum = 0\n",
    "    total_batches = 0\n",
    "    target_match_sum = []\n",
    "\n",
    "    #Exact match\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        object_embeddings = batch.object_embeddings.to(device)\n",
    "        edge_index1 = batch.edge_index1.to(device) #relation \n",
    "        edge_attr1 = batch.edge_attr1.to(device)\n",
    "        edge_index2 = batch.edge_index2.to(device) #similarity\n",
    "        edge_attr2 = batch.edge_attr2.to(device)\n",
    "        edge_index3 = batch.edge_index3.to(device) #neighbor\n",
    "        edge_attr3 = batch.edge_attr3.to(device)\n",
    "        question_embedding = batch.question_embedding.to(device)\n",
    "        target = batch.target.to(device)\n",
    "\n",
    "        logits = model(object_embeddings, edge_index1, edge_attr1, edge_index2, edge_attr2)\n",
    "        loss = loss_fn(logits.squeeze(), target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Performance of every element(every obj)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        preds = (probabilities > 0.5).float()\n",
    "        exact_match = calculate_exact_match_ratio(target, preds)\n",
    "        exact_match_sum += exact_match\n",
    "        # print(logits)\n",
    "        # print(loss)\n",
    "        # print(probabilities)\n",
    "\n",
    "        # if question has the true answer comupte the RECALL\n",
    "        if torch.sum(target).item()>0:\n",
    "            target_match = calculate_target_match_ratio(target, preds)\n",
    "            target_match_sum.append(target_match)\n",
    "            # print(f'target match:{target_match}, exact_match:{exact_match}')\n",
    "\n",
    "        # Performance of Exactly match of qa(obj list)\n",
    "        predict_list.append(preds.squeeze().to(\"cpu\").tolist())\n",
    "        target_list.append(target.to(\"cpu\").to(torch.int).tolist())\n",
    "\n",
    "        total_batches += 1\n",
    "                        \n",
    "    #performance of obj\n",
    "    avg_loss = running_loss / total_batches\n",
    "    avg_acc = exact_match_sum / total_batches\n",
    "    avg_acc_pos = sum(target_match_sum) / len(target_match_sum) if target_match_sum else 0.0\n",
    "    #performance of exact match of obj list\n",
    "    epoch_accu = calculate_exact_match_ratio(target_list, predict_list)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}],Train Avg Loss: {avg_loss:.4f}, Train Accuracy: {avg_acc:.4f}, Train Recall: {avg_acc_pos:.4f}, Exact match:{epoch_accu:.4f}\")\n",
    "\n",
    "    val_acc = validate(model, val_dataloader)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved with avg of Exact match: {best_val_acc:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T15:20:04.471918Z",
     "iopub.status.idle": "2025-02-02T15:20:04.472464Z",
     "shell.execute_reply": "2025-02-02T15:20:04.472222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PygtestData(Data):\n",
    "    def __init__(self, question_embedding, object_embeddings, edge_index1, edge_attr1, edge_index2, edge_attr2, edge_index3, edge_attr3, global_id):\n",
    "        super().__init__()\n",
    "        self.question_embedding = question_embedding\n",
    "        self.object_embeddings = object_embeddings\n",
    "        self.edge_index1 = edge_index1\n",
    "        self.edge_attr1 = edge_attr1\n",
    "        self.edge_index2 = edge_index2\n",
    "        self.edge_attr2 = edge_attr2\n",
    "        self.edge_index3 = edge_index3\n",
    "        self.edge_attr3 = edge_attr3\n",
    "        self.global_id = global_id\n",
    "\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        # Increment indices for edge_index1 and edge_index2 (pyg data: only 1 edge default)\n",
    "        if key in ['edge_index1', 'edge_index2', 'edge_index3']:\n",
    "            return self.object_embeddings.size(0)  # Increment by the number of nodes\n",
    "        return super().__inc__(key, value, *args, **kwargs)\n",
    "        \n",
    "class testDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, question_embed, object_embeddings, edges_per_doc, sim_per_doc, neighbor_per_doc):\n",
    "        self.question_embeddings = question_embed.question\n",
    "        self.qa_doc_ids = question_embed.pmcid\n",
    "        # self.obj_doc = object_embeddings.doc\n",
    "        # self.object_embeddings = object_embeddings.embeddings\n",
    "        self.obj_doc = object_embeddings\n",
    "        self.edges_per_doc = edges_per_doc\n",
    "        self.sim_per_doc = sim_per_doc\n",
    "        self.neighbor_per_doc = neighbor_per_doc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.question_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' \n",
    "        we have to make graph of every question and the objects, so dataloader helped indict the question using idx and we match the doc to find the objects\n",
    "        '''\n",
    "        question_embedding = self.question_embeddings[idx].float()\n",
    "        answer_doc_id = self.qa_doc_ids[idx]\n",
    "        \n",
    "        answer_doc_id = str(answer_doc_id)\n",
    "        \n",
    "        # qa --> doc_id -->edges\n",
    "        doc_edges = self.edges_per_doc[answer_doc_id]\n",
    "        doc_sim = self.sim_per_doc[answer_doc_id]\n",
    "        doc_neighbor = self.neighbor_per_doc[answer_doc_id]\n",
    "\n",
    "        # edges --> edges index and attr\n",
    "        edge_index1 = torch.tensor([[edge[0], edge[1]] for edge in doc_edges], dtype=torch.long).t().contiguous()\n",
    "        edge_attr1 = torch.tensor([edge[2] for edge in doc_edges], dtype=torch.long)\n",
    "\n",
    "        edge_index2 = torch.tensor([[edge[0], edge[1]] for edge in doc_sim], dtype=torch.long).t().contiguous()\n",
    "        edge_attr2 = torch.tensor([edge[2] for edge in doc_sim], dtype=torch.float)\n",
    "\n",
    "        edge_index3 = torch.tensor([[edge[0], edge[1]] for edge in doc_neighbor], dtype=torch.long).t().contiguous()\n",
    "        edge_attr3 = torch.tensor([edge[2] for edge in doc_neighbor], dtype=torch.long)\n",
    "        \n",
    "        # doc_id --> node feature\n",
    "        doc_mask = self.obj_doc['doc'] == answer_doc_id\n",
    "        filtered_embeddings = self.obj_doc.loc[doc_mask, 'embeddings']\n",
    "        object_embeddings = torch.stack(list(filtered_embeddings)).squeeze(1).float()  #[x, 4096]\n",
    "\n",
    "        # add QUESTION NODE at the end \n",
    "        qa_embeddings = torch.cat([object_embeddings, question_embedding], dim=0) \n",
    "        \n",
    "        # Index of the QUESTION NODE\n",
    "        question_node_idx = len(filtered_embeddings)  \n",
    "        num_objects = len(filtered_embeddings)\n",
    "        question_to_object_edges = torch.tensor([[question_node_idx] * num_objects, torch.arange(num_objects)]).long()\n",
    "        \n",
    "        edge_index1 = torch.cat([edge_index1, question_to_object_edges], dim=1)\n",
    "        edge_index2 = torch.cat([edge_index2, question_to_object_edges], dim=1)\n",
    "        edge_index3 = torch.cat([edge_index3, question_to_object_edges], dim=1)\n",
    "\n",
    "        # attr of the QUESTION NODE \n",
    "        similarities_question_to_object = torch.matmul(question_embedding, object_embeddings.t()).squeeze(0)  # 点乘计算相似度\n",
    "        similarities_question_to_object = torch.tensor(similarities_question_to_object, dtype=torch.float)\n",
    "        edge_attr_question_to_object = torch.ones(len(filtered_embeddings),  dtype=torch.float)\n",
    "        \n",
    "        edge_attr1 = torch.cat([edge_attr1, edge_attr_question_to_object], dim=0)\n",
    "        edge_attr2 = torch.cat([edge_attr2, similarities_question_to_object], dim=0)        \n",
    "        edge_attr3 = torch.cat([edge_attr3, edge_attr_question_to_object], dim=0)\n",
    "\n",
    "        #global_id\n",
    "        global_id = self.obj_doc.loc[doc_mask, 'global_id'].values\n",
    "        global_id = torch.tensor(global_id)\n",
    "\n",
    "        return PygtestData(question_embedding, qa_embeddings, edge_index1, edge_attr1, edge_index2, edge_attr2, edge_index3, edge_attr3, global_id)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T15:20:04.473305Z",
     "iopub.status.idle": "2025-02-02T15:20:04.473784Z",
     "shell.execute_reply": "2025-02-02T15:20:04.473588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test(model, val_dataloader):\n",
    "    model.eval()\n",
    "    exact_match_sum = 0\n",
    "    target_match_sum = []\n",
    "    total_batches = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    predict_list = []\n",
    "    global_id_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            object_embeddings = batch.object_embeddings.to(device)\n",
    "            edge_index1 = batch.edge_index1.to(device) #relation \n",
    "            edge_attr1 = batch.edge_attr1.to(device)\n",
    "            edge_index2 = batch.edge_index2.to(device) #similarity\n",
    "            edge_attr2 = batch.edge_attr2.to(device)\n",
    "            edge_index3 = batch.edge_index3.to(device) #neighbor\n",
    "            edge_attr3 = batch.edge_attr3.to(device)\n",
    "            question_embedding = batch.question_embedding.to(device)\n",
    "            global_id = batch.global_id.to(device)\n",
    "    \n",
    "            # optimizer.zero_grad()\n",
    "            logits = model(object_embeddings, edge_index1, edge_attr1, edge_index2, edge_attr2)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            preds = (probabilities > 0.5).float()\n",
    "            \n",
    "            predict_list.append(preds.squeeze().to(\"cpu\").tolist())\n",
    "            global_id_list.append(global_id.to(\"cpu\").to(torch.int).tolist())\n",
    "\n",
    "    return predict_list, global_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T15:20:04.476393Z",
     "iopub.status.idle": "2025-02-02T15:20:04.476885Z",
     "shell.execute_reply": "2025-02-02T15:20:04.476682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extracting(pre_ids, global_ids):\n",
    "  outputs = []\n",
    "  for j, pids in enumerate(pre_ids):\n",
    "    output = []\n",
    "    gids = global_ids[j]\n",
    "    for i,pid in enumerate(pids):\n",
    "      if pid == 1:\n",
    "        output.append(gids[i])\n",
    "    outputs.append(output)\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T15:20:04.478327Z",
     "iopub.status.idle": "2025-02-02T15:20:04.478800Z",
     "shell.execute_reply": "2025-02-02T15:20:04.478599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# best_model_path = '/kaggle/input/rela-sim-graph/relasim-graph.pth'\n",
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T15:20:04.479964Z",
     "iopub.status.idle": "2025-02-02T15:20:04.480452Z",
     "shell.execute_reply": "2025-02-02T15:20:04.480217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "global_id_list, relation_list, position_list = combine_infomation(test_doc_info)\n",
    "test_doc_emb['global_id'] = global_id_list\n",
    "test_doc_emb['relations'] = relation_list\n",
    "test_doc_emb['neighbor'] = position_list\n",
    "\n",
    "grouped = test_doc_emb.groupby('doc')\n",
    "\n",
    "test_edges_per_doc = {}\n",
    "test_sim_per_doc = {}\n",
    "test_neighbor_per_doc = {}\n",
    "\n",
    "for doc_id, group in tqdm(grouped):\n",
    "    doc_embeddings = torch.cat([torch.tensor(e) for e in group['embeddings']], dim=0)    \n",
    "    doc_relations = group['relations']\n",
    "    doc_neighbor = group['neighbor']\n",
    "    test_edges_per_doc[doc_id] = relation_edges(doc_embeddings, doc_relations)\n",
    "    test_sim_per_doc[doc_id] = similarity_edges(doc_embeddings, top_k_smilarity)\n",
    "    test_neighbor_per_doc[doc_id] = relation_edges(doc_embeddings, doc_neighbor)\n",
    "\n",
    "test_dataset = testDataset(test_question_emb, test_doc_emb, test_edges_per_doc, test_sim_per_doc, test_neighbor_per_doc)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T15:20:04.481363Z",
     "iopub.status.idle": "2025-02-02T15:20:04.481838Z",
     "shell.execute_reply": "2025-02-02T15:20:04.481627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predict_list, global_id_list = test(model, test_dataloader)\n",
    "output = extracting(predict_list, global_id_list)\n",
    "\n",
    "new_output = []\n",
    "for i in range(len(output)):\n",
    "    if output[i] == []:\n",
    "        new_output.append([-1])\n",
    "    elif -1 in output[i] and len(output[i]) > 1:\n",
    "        correct_elements = []\n",
    "        for e in output[i]:\n",
    "            if e != -1:\n",
    "                correct_elements.append(e)\n",
    "        new_output.append(str(correct_elements))\n",
    "    else:\n",
    "        new_output.append(str(output[i]))\n",
    "\n",
    "output = new_output\n",
    "\n",
    "id_list = range(0, len(output))\n",
    "df = pd.DataFrame(id_list, columns=['id'])\n",
    "df['answer'] = output\n",
    "df.to_csv(f'/kaggle/working/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6096336,
     "sourceId": 9919641,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6130754,
     "sourceId": 9966094,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6482170,
     "sourceId": 10469185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6482173,
     "sourceId": 10469188,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
